{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "pip-requirements"
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade pip\n",
    "pip install langchain\n",
    "# pip install openai\n",
    "pip install pandas\n",
    "pip install tabulate\n",
    "pip install transformers\n",
    "# pip install llama-cpp-python\n",
    "pip install llama-cpp-python==0.1.48\n",
    "# install agent tools\n",
    "pip install google-search-results\n",
    "pip install wikipedia\n",
    "# playwright tool\n",
    "pip install playwright\n",
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(False): # run the following code to download a model like wizardLM-7B.ggml.q4_0.bin from huggingface.co\n",
    "  l7b=\"https://huggingface.co/hlhr202/llama-7B-ggml-int4/resolve/main/ggml-model-q4_0.bin\"\n",
    "#   l13b=\"https://huggingface.co/Drararara/llama-13B-ggml/resolve/main/ggml-model-q4_0.bin\"   \n",
    "#   a7b=\"https://huggingface.co/hlhr202/alpaca-7B-ggml-int4/resolve/main/ggml-alpaca-7b-q4.bin\"\n",
    "#   a13b=\"https://huggingface.co/Pi3141/alpaca-lora-13B-ggml/resolve/main/ggml-model-q4_0.bin\"  \n",
    "#   o13b=\"https://huggingface.co/Black-Engineer/oasst-llama13b-ggml-q4/resolve/main/qunt4_0.bin\" \n",
    "#   g4a=\"https://huggingface.co/eachadea/ggml-gpt4all-7b-4bit/resolve/main/gpt4all-lora-quantized-ggml.bin\"\n",
    "#   k7b=\"https://huggingface.co/TheBloke/koala-7B-GPTQ-4bit-128g-GGML/resolve/main/koala-7B-4bit-128g.GGML.bin\" \n",
    "#   v7b=\"https://huggingface.co/eachadea/ggml-vicuna-7b-1.1/resolve/main/ggml-vicuna-7b-1.1-q4_0.bin\"\n",
    "#   v13b=\"https://huggingface.co/eachadea/ggml-vicuna-13b-1.1/resolve/main/ggml-vicuna-13b-1.1-q4_0.bin\"\n",
    "#   wizVic=\"https://huggingface.co/TheBloke/wizard-vicuna-13B-GGML/resolve/main/wizard-vicuna-13B.ggml.q4_0.bin\" \n",
    "#   gpt4Vicuna=\"https://huggingface.co/TheBloke/gpt4-x-vicuna-13B-GGML/resolve/main/gpt4-x-vicuna-13B.ggml.q4_0.bin\"\n",
    "  \n",
    "  weights=requests.get(l7b)\n",
    "  with open(\"weights.bin\",\"wb\") as out_file:\n",
    "    out_file.write(weights.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "llamallm = Llama(model_path=\"D:\\Workspace\\LangChain\\ggml-model-q4_0.bin\",n_ctx=2048)\n",
    "output = llamallm(\"Software developer\", echo=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLLM(LLM):  \n",
    "  def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:    \n",
    "    print(\"***\\n\"+prompt+\"\\n***\")\n",
    "    output = llamallm(prompt, echo=False) #, stop=[\"Q:\", \"\\n\"], max_tokens=100,     \n",
    "    output = output[\"choices\"][0][\"text\"]\n",
    "    output = re.sub(\"\\nAction:(.*)[Dd]atabase(.*)\",\"\\nAction: Database\",output)    \n",
    "    output = re.sub(\"\\nAction:(.*)Wikipedia(.*)\",\"\\nAction: Wikipedia\",output)\n",
    "    if(output.find(\"\\nAction:\")>=0 and output.find(\"\\nObservation:\")>output.find(\"\\nAction:\")): return(output[0:output.find(\"\\nObservation:\")])\n",
    "    else: return(output)\n",
    "  @property\n",
    "  def _llm_type(self) -> str:\n",
    "    return \"custom\"\n",
    "\n",
    "llm=CustomLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import PlayWrightBrowserToolkit\n",
    "from langchain.tools.playwright.utils import (\n",
    "    create_async_playwright_browser,\n",
    ")\n",
    "\n",
    "async_browser = create_async_playwright_browser()\n",
    "playwrighttoolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)\n",
    "playwrighttools = playwrighttoolkit.get_tools()\n",
    "\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"\"\n",
    "tools = load_tools([\"requests_get\", \"serpapi\", \"python_repl\", \"wikipedia\"], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(playwrighttools + tools, llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "agent.run(\"Write the code for index website page like https://langchain.com/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
